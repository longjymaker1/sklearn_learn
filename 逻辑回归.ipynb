{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0逻辑回归\n",
    "\n",
    "逻辑回归是一种分类算法，它是基于线性回归的分类算法，所以要先了解一下线性回归，线性回归公式\n",
    "$$z=\\theta_{0}+\\theta_{1}x_{1}+\\theta_{2}x_{2}+...+\\theta_{n}x_{n}$$\n",
    "$\\theta$被统称为模型参数，其中$\\theta_{0}$为截距`(intercept)`，$\\theta_{1}$-$\\theta_{n}$被称为系数`(coefficient)`，其实与小学的y=ax+b是相同的性质。还可以用矩阵的形式表的，其中x和$\\theta$都可以看做一个列矩阵\n",
    "$$z=[\\theta_{0}, \\theta_{1},\\theta_{2}...\\theta_{n}]*\n",
    "    \\begin{bmatrix}\n",
    "    x_{0} \\\\\n",
    "    x_{1} \\\\\n",
    "    x_{2} \\\\\n",
    "    ... \\\\\n",
    "    x_{n} \\\\\n",
    "    \\end{bmatrix}\n",
    "    =\\theta^Tx \\text {    ($x_{0}$=1)}$$\n",
    "    \n",
    "线性回归的任务是构造一个预测韩式$z$来映射输入的特征矩阵x和标签值y的关系，而**构造预测函数的核心就是找出模型的参数: $\\theta_{0}$和$\\theta^T$**,著名的最小二乘法就是用来求解线性回归中参数的数学方法。\n",
    "\n",
    "通过函数$z$，线性回归使用输入的特征矩阵x来输出一组连续的标签值y_pred,来完成各种预测连续型变量的任务(比如销售预测，估价预测等)。如果，标签是离散型变量，尤其是0-1分布的离散变量，可以引入联系函数(link function),将现行回归方程z转变为g(z)，g(z)是可微函数,并且令g(z)的值分布在(0,1)之间，当g(z)接近0的样本的标签为类别0，当g(z)接近1的样本的标签为类别1，这样就是一个分类模型。这个联系函数对于逻辑回归来说就是`Sigmoid函数`\n",
    "$$g(z) = \\frac{1}{1+e^{-z}}$$\n",
    "线性回归中$z=\\theta^Tx$带入到联系函数Sigmoid中得到了二元逻辑回归模型的一般形式：\n",
    "$$g(z) = y(x) = \\frac{1}{1+e^{-\\theta^Tx}}$$\n",
    "而$g(z)$就是逻辑回归返回的标签值。此时$y(x)$的取值在[0,1]之间，此时$y(x)+(1-y(x))=1$必然成立。如果令$y(x)$除以$1-y(x)$就得到形式几率(odds)的$\\frac{y(x)}{1-y(x)}$,再次基础上取对数，可以得到:\n",
    "$$ln\\frac{y(x)}{1-y(x)} = ln(\\frac{\\frac{1}{1+e^{-\\theta^Tx}}}{1-\\frac{1}{1+e^{-\\theta^Tx}}})$$\n",
    "$$ = ln(\\frac{\\frac{1}{1+e^{-\\theta^Tx}}}{\\frac{e^{-\\theta^Tx}}{1+e^{-\\theta^Tx}}})$$\n",
    "$$ = ln(\\frac{1}{e^{-\\theta^Tx}})$$\n",
    "$$ = ln(e^{\\theta^Tx})$$\n",
    "$$ = \\theta^Tx$$\n",
    "\n",
    "不难发现，y(x)的形似几率取对数的本质其实就是线性回归z，实际上是在线性回归模型预测结果取对数几率来让其的结果无限笔记0或1。因此，其对应的模型被称为“对数几率回归”(logistic Regression),也就是逻辑回归\n",
    "\n",
    "> $y(x) = ln\\frac{y(x)}{1-y(x)}$代表了样本的某一类标签的概率吗？\\\n",
    "$ln\\frac{y(x)}{1-y(x)}$是形似对数几率的一种变化。而几率odds的本质是$frac{p}{1-p}$,其中p是事件A发送的概率，而1-p是事件A不发生的概率，并且$p+(1-p)=1$。因此，很多人理解逻辑回归是对y(x)做如下解释\\\n",
    "\\\n",
    "让线性回归结果逼近0和1，此时y(x)和(1-y(x))之和为1，因此它可以被看做是一对正反例发生的概率，即y(x)是某样本i的标签被预测为1的概率，而1-y(x)是被预测为0的概率，$ln\\frac{y(x)}{1-y(x)}$就是i被预测为1的相对概率。基于这种理解，可以使用最大似然法和gail分布函数推导出逻辑回归的损失函数，并且吧返回样本在标签取值上的概率当做是逻辑回归的性质来使用，当求概率的时候都会使用逻辑回归\n",
    "\n",
    "## 1.2逻辑回归优点：\n",
    "* 对线性关系的数据拟合效果好到丧心病狂\n",
    "* 计算速度快\n",
    "* 逻辑回归返回的不是0或1，而是一个类概率的小数\n",
    "* 抗噪能力较强\n",
    "\n",
    "## 1.3 sklearn中的逻辑回归\n",
    "| 类 | 说明 |\n",
    "| :------: | :------: |\n",
    "| linear_model.LogisticRegression | 逻辑回归回归分离器(又叫logit回归，最大熵分离器) |\n",
    "| linear_model.LogisticRegressionCV | 带交叉验证的逻辑回归分离器 |\n",
    "| linear_model.logistic_regression_path | 计算Logistic回归模型以获得正则化参数列表 |\n",
    "| linear_model.SGDClassifier | 利用梯度下架求解线性分离器(SVM,逻辑回归等) |\n",
    "| linear_model.SGDRegressor | 利用梯度下降最小化正则化后的损失函数的线性回归模型 |\n",
    "| metrics.log_loss | 对数损失，又称逻辑损失或交叉熵损失 |\n",
    "|  |  |\n",
    "| 其他会涉及的类 | 说明 |\n",
    "| metrics.confusion_matrix | 混淆矩阵，模型评估指标之一 |\n",
    "| metrics.roc_auc_score | ROC曲线，模型评估指标之一 |\n",
    "| metrics.accuracy_score | 精确性，模型评估指标之一 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. linear_model.LogisticRegression\n",
    "\n",
    "class sklearn.linear_model.LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "\n",
    "## 2.1 二元逻辑回归的损失函数\n",
    "### 2.1.1 损失函数概念\n",
    "在决策树和随机森林中，追求的是测试集表现最优，但是逻辑回归是基于训练集求解$\\theta$, 并且希望尽可能拟合数据。\n",
    "\n",
    "因此，使用“**损失函数**”这个指标，来**衡量参数为$\\theta$的模型拟合训练集时产生损失信息的大小，并以此来衡量$\\theta$的优劣**,如果损失值函数的值大越不好，反之损失函数越小越好\n",
    "\n",
    "> 所有没有“求解参数”需求的模型都没有损失函数\n",
    "\n",
    "逻辑回归的损失函数是由极大似然估计推到出来的，$$J(\\theta)=-\\sum_{i=1}^m(y_{i}*log(y_{\\theta}(x_{i})) + (1-y_{i}) * log(1-y_{\\theta}(x_{i})))$$\n",
    "其中,$\\theta$表示求解出来的一组参数，m是样本数量，$y_{i}$是样本i上的真实标签，$y_{\\theta}(x_{i})$是样本i上，基于参数$\\theta$计算出来的逻辑回归的返回值，$x_{i}$是样本i各个特征的取值。目标是求解出是$J(\\theta)$最小的$\\theta$取值。\n",
    "> 注意，在逻辑回归的本质函数$y(x)$里特征矩阵x是自变量，参数是$\\theta$。但在损失函数中，参数$\\theta$是损失函数的自变量，x和y都是已知特征矩阵和标签，相当于损失函数的参数。\n",
    "\n",
    "## 2.2 重要参数penalty & C\n",
    "### 2.2.1 正则化\n",
    "正则化是用来防止过拟合的过程，常用的有`L1`和`L2`两种选项,L1用来控制稀疏性，L2用来控制稠密性，如果一个参数对结果的贡献很小，当L1变小(正则化加强)时就会把这个参数变为0，而L2不会，只会让这次参数趋近0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrl1 = LR(penalty='l1', solver='liblinear', C=0.5, max_iter=1000)\n",
    "lrl2 = LR(penalty='l2', solver='liblinear', C=0.5, max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.98472344,  0.03125096, -0.13502977, -0.01618692,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.50302337,  0.        , -0.07122535,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        , -0.24507962, -0.12829203, -0.01443522,  0.        ,\n",
       "         0.        , -2.05760956,  0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 逻辑回归的重要属性coef_,查看每个特征对应的参数\n",
    "lrl1 = lrl1.fit(X,y)\n",
    "lrl1.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(lrl1.coef_ != 0).sum(axis=1)  # 有10个不为0的系数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.58651399e+00,  1.05063447e-01,  4.48683632e-02,\n",
       "        -3.74375536e-03, -8.56901613e-02, -2.94287943e-01,\n",
       "        -4.37733190e-01, -2.07600072e-01, -1.22519137e-01,\n",
       "        -1.87465544e-02,  2.78262183e-02,  8.41924650e-01,\n",
       "         1.66118667e-01, -9.75336736e-02, -8.75079523e-03,\n",
       "        -3.14114111e-02, -6.23724835e-02, -2.55180879e-02,\n",
       "        -2.58371386e-02, -1.14472251e-03,  1.34078040e+00,\n",
       "        -3.01592084e-01, -1.79752694e-01, -2.25790709e-02,\n",
       "        -1.57048116e-01, -8.65467461e-01, -1.12967239e+00,\n",
       "        -3.98774604e-01, -3.79997636e-01, -8.51004287e-02]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrl2 = lrl2.fit(X,y)\n",
    "lrl2.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L1 正则化会吧参数压缩到0，L2会让每个参数都有贡献"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
