{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0逻辑回归\n",
    "\n",
    "逻辑回归是一种分类算法，它是基于线性回归的分类算法，所以要先了解一下线性回归，线性回归公式\n",
    "$$z=\\theta_{0}+\\theta_{1}x_{1}+\\theta_{2}x_{2}+...+\\theta_{n}x_{n}$$\n",
    "$\\theta$被统称为模型参数，其中$\\theta_{0}$为截距`(intercept)`，$\\theta_{1}$-$\\theta_{n}$被称为系数`(coefficient)`，其实与小学的y=ax+b是相同的性质。还可以用矩阵的形式表的，其中x和$\\theta$都可以看做一个列矩阵\n",
    "$$z=[\\theta_{0}, \\theta_{1},\\theta_{2}...\\theta_{n}]*\n",
    "    \\begin{bmatrix}\n",
    "    x_{0} \\\\\n",
    "    x_{1} \\\\\n",
    "    x_{2} \\\\\n",
    "    ... \\\\\n",
    "    x_{n} \\\\\n",
    "    \\end{bmatrix}\n",
    "    =\\theta^Tx \\text {    ($x_{0}$=1)}$$\n",
    "    \n",
    "线性回归的任务是构造一个预测韩式$z$来映射输入的特征矩阵x和标签值y的关系，而**构造预测函数的核心就是找出模型的参数: $\\theta_{0}$和$\\theta^T$**,著名的最小二乘法就是用来求解线性回归中参数的数学方法。\n",
    "\n",
    "通过函数$z$，线性回归使用输入的特征矩阵x来输出一组连续的标签值y_pred,来完成各种预测连续型变量的任务(比如销售预测，估价预测等)。如果，标签是离散型变量，尤其是0-1分布的离散变量，可以引入联系函数(link function),将现行回归方程z转变为g(z)，g(z)是可微函数,并且令g(z)的值分布在(0,1)之间，当g(z)接近0的样本的标签为类别0，当g(z)接近1的样本的标签为类别1，这样就是一个分类模型。这个联系函数对于逻辑回归来说就是`Sigmoid函数`\n",
    "$$g(z) = \\frac{1}{1+e^{-z}}$$\n",
    "线性回归中$z=\\theta^Tx$带入到联系函数Sigmoid中得到了二元逻辑回归模型的一般形式：\n",
    "$$g(z) = y(x) = \\frac{1}{1+e^{-\\theta^Tx}}$$\n",
    "而$g(z)$就是逻辑回归返回的标签值。此时$y(x)$的取值在[0,1]之间，此时$y(x)+(1-y(x))=1$必然成立。如果令$y(x)$除以$1-y(x)$就得到形式几率(odds)的$\\frac{y(x)}{1-y(x)}$,再次基础上取对数，可以得到:\n",
    "$$ln\\frac{y(x)}{1-y(x)} = ln(\\frac{\\frac{1}{1+e^{-\\theta^Tx}}}{1-\\frac{1}{1+e^{-\\theta^Tx}}})$$\n",
    "$$ = ln(\\frac{\\frac{1}{1+e^{-\\theta^Tx}}}{\\frac{e^{-\\theta^Tx}}{1+e^{-\\theta^Tx}}})$$\n",
    "$$ = ln(\\frac{1}{e^{-\\theta^Tx}})$$\n",
    "$$ = ln(e^{\\theta^Tx})$$\n",
    "$$ = \\theta^Tx$$\n",
    "\n",
    "不难发现，y(x)的形似几率取对数的本质其实就是线性回归z，实际上是在线性回归模型预测结果取对数几率来让其的结果无限笔记0或1。因此，其对应的模型被称为“对数几率回归”(logistic Regression),也就是逻辑回归\n",
    "\n",
    "> $y(x) = ln\\frac{y(x)}{1-y(x)}$代表了样本的某一类标签的概率吗？\\\n",
    "$ln\\frac{y(x)}{1-y(x)}$是形似对数几率的一种变化。而几率odds的本质是$frac{p}{1-p}$,其中p是事件A发送的概率，而1-p是事件A不发生的概率，并且$p+(1-p)=1$。因此，很多人理解逻辑回归是对y(x)做如下解释\\\n",
    "\\\n",
    "让线性回归结果逼近0和1，此时y(x)和(1-y(x))之和为1，因此它可以被看做是一对正反例发生的概率，即y(x)是某样本i的标签被预测为1的概率，而1-y(x)是被预测为0的概率，$ln\\frac{y(x)}{1-y(x)}$就是i被预测为1的相对概率。基于这种理解，可以使用最大似然法和gail分布函数推导出逻辑回归的损失函数，并且吧返回样本在标签取值上的概率当做是逻辑回归的性质来使用，当求概率的时候都会使用逻辑回归\n",
    "\n",
    "## 1.2逻辑回归优点：\n",
    "* 对线性关系的数据拟合效果好到丧心病狂\n",
    "* 计算速度快\n",
    "* 逻辑回归返回的不是0或1，而是一个类概率的小数\n",
    "* 抗噪能力较强\n",
    "\n",
    "## 1.3 sklearn中的逻辑回归\n",
    "| 类 | 说明 |\n",
    "| :------: | :------: |\n",
    "| linear_model.LogisticRegression | 逻辑回归回归分离器(又叫logit回归，最大熵分离器) |\n",
    "| linear_model.LogisticRegressionCV | 带交叉验证的逻辑回归分离器 |\n",
    "| linear_model.logistic_regression_path | 计算Logistic回归模型以获得正则化参数列表 |\n",
    "| linear_model.SGDClassifier | 利用梯度下架求解线性分离器(SVM,逻辑回归等) |\n",
    "| linear_model.SGDRegressor | 利用梯度下降最小化正则化后的损失函数的线性回归模型 |\n",
    "| metrics.log_loss | 对数损失，又称逻辑损失或交叉熵损失 |\n",
    "|  |  |\n",
    "| 其他会涉及的类 | 说明 |\n",
    "| metrics.confusion_matrix | 混淆矩阵，模型评估指标之一 |\n",
    "| metrics.roc_auc_score | ROC曲线，模型评估指标之一 |\n",
    "| metrics.accuracy_score | 精确性，模型评估指标之一 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. linear_model.LogisticRegression\n",
    "\n",
    "class sklearn.linear_model.LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
